{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "915bcc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "76e2074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTION_MAPPING_PT_TO_EN = {\n",
    "    'felicidade': 'Happiness',\n",
    "    'tristeza': 'Sadness',\n",
    "    'medo': 'Fear',\n",
    "    'raiva': 'Anger',\n",
    "    'surpresa': 'Surprise',\n",
    "    'desgosto': 'Disgust',\n",
    "    'neutro': 'Neutral',\n",
    "}\n",
    "\n",
    "CANONICAL_LABELS = set(EMOTION_MAPPING_PT_TO_EN.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5aff9f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ground_truth(gt_path: str) -> Dict[str, Dict[str, str]]:\n",
    "    \"\"\"Load ground truth JSON mapping to dict[output][segment] -> label (EN).\"\"\"\n",
    "    with open(gt_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    gt: Dict[str, Dict[str, str]] = {}\n",
    "    for output_name, segments in data.items():\n",
    "        gt[output_name] = {}\n",
    "        for seg_id, seg_data in segments.items():\n",
    "            label = seg_data.get('principal_emocao_detectada')\n",
    "            gt[output_name][seg_id] = label\n",
    "    return gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "58cefe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label(value: Optional[str]) -> Optional[str]:\n",
    "    if not value:\n",
    "        return None\n",
    "\n",
    "    raw = str(value).strip().lower()\n",
    "\n",
    "    replacements = {\n",
    "        'emoção': 'emocao',\n",
    "        'felicidade': 'felicidade',\n",
    "        'tristeza': 'tristeza',\n",
    "        'medo': 'medo',\n",
    "        'raiva': 'raiva',\n",
    "        'surpresa': 'surpresa',\n",
    "        'desgosto': 'desgosto',\n",
    "        'neutro': 'neutro',\n",
    "    }\n",
    "\n",
    "    if raw in EMOTION_MAPPING_PT_TO_EN:\n",
    "        return EMOTION_MAPPING_PT_TO_EN[raw]\n",
    "\n",
    "    alias = {\n",
    "        'happy': 'Happiness',\n",
    "        'happiness': 'Happiness',\n",
    "        'sad': 'Sadness',\n",
    "        'sadness': 'Sadness',\n",
    "        'fear': 'Fear',\n",
    "        'afraid': 'Fear',\n",
    "        'angry': 'Anger',\n",
    "        'anger': 'Anger',\n",
    "        'surprise': 'Surprise',\n",
    "        'surprised': 'Surprise',\n",
    "        'disgust': 'Disgust',\n",
    "        'disgusted': 'Disgust',\n",
    "        'neutral': 'Neutral',\n",
    "    }\n",
    "    if raw in alias:\n",
    "        return alias[raw]\n",
    "\n",
    "    cap = raw.capitalize()\n",
    "    if cap in CANONICAL_LABELS:\n",
    "        return cap\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3933f618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_splits(csv_path: str) -> Dict[Tuple[str, str], str]:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    mapping: Dict[Tuple[str, str], str] = {}\n",
    "    for _, row in df.iterrows():\n",
    "        file_path = str(row['File Path'])\n",
    "        split = str(row['Split']).strip().lower()\n",
    "        file_path = file_path.replace('/', '\\\\').replace('\\\\', os.sep)\n",
    "        parts = file_path.split(os.sep)\n",
    "        if len(parts) >= 2:\n",
    "            output_name = parts[0]\n",
    "            seg_with_ext = parts[1]\n",
    "            seg = os.path.splitext(seg_with_ext)[0]\n",
    "            mapping[(output_name, seg)] = split\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dd34552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pred_label(seg_obj: dict) -> Optional[str]:\n",
    "    for key in (\n",
    "        'principal_emocao_detectada',\n",
    "        'emoção',\n",
    "        'emocao',\n",
    "        'emotion',\n",
    "        'label',\n",
    "        'pred',\n",
    "    ):\n",
    "        if key in seg_obj and seg_obj[key]:\n",
    "            return str(seg_obj[key])\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "070c740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_predictions(all_json_path: str) -> Dict[str, Dict[str, Optional[str]]]:\n",
    "    with open(all_json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    preds: Dict[str, Dict[str, Optional[str]]] = {}\n",
    "    for output_name, segments in data.items():\n",
    "        if not isinstance(segments, dict):\n",
    "            continue\n",
    "        preds[output_name] = {}\n",
    "        for seg_id, seg_obj in segments.items():\n",
    "            if not isinstance(seg_obj, dict):\n",
    "                continue\n",
    "            raw_label = extract_pred_label(seg_obj)\n",
    "            preds[output_name][seg_id] = normalize_label(raw_label)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da523f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_pairs(gt: Dict[str, Dict[str, str]],\n",
    "                  preds: Dict[str, Dict[str, Optional[str]]],\n",
    "                  subset: Optional[str],\n",
    "                  split_map: Dict[Tuple[str, str], str]) -> Tuple[List[str], List[str], int]:\n",
    "    y_true: List[str] = []\n",
    "    y_pred: List[str] = []\n",
    "    hallucinations = 0\n",
    "\n",
    "    for output_name, segs_gt in gt.items():\n",
    "        if output_name not in preds:\n",
    "            continue\n",
    "        segs_pred = preds[output_name]\n",
    "        for seg_id, true_label in segs_gt.items():\n",
    "            if seg_id not in segs_pred:\n",
    "                continue\n",
    "            if subset == 'test':\n",
    "                if split_map.get((output_name, seg_id), None) != 'test':\n",
    "                    continue\n",
    "            pred_label = segs_pred.get(seg_id)\n",
    "            if pred_label is None:\n",
    "                hallucinations += 1\n",
    "                # Skip adding to y_true/y_pred so sklearn doesn't see None labels\n",
    "                continue\n",
    "            y_true.append(true_label)\n",
    "            y_pred.append(pred_label)\n",
    "\n",
    "    return y_true, y_pred, hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2fa1466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true: List[str], y_pred: List[str],\n",
    "                    hallucinations: int, total_seen: int) -> Dict[str, float]:\n",
    "    if len(y_true) == 0:\n",
    "        return {\n",
    "            'precision_weighted': 0.0,\n",
    "            'recall_weighted': 0.0,\n",
    "            'f1_weighted': 0.0,\n",
    "            'precision_macro': 0.0,\n",
    "            'recall_macro': 0.0,\n",
    "            'f1_macro': 0.0,\n",
    "            'accuracy': 0.0,\n",
    "            'samples': 0,\n",
    "            'hallucination_rate': 0.0,\n",
    "            'coverage': 0.0,\n",
    "        }\n",
    "\n",
    "    pw, rw, fw, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    pm, rm, fm, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    rate = (hallucinations / total_seen) if total_seen > 0 else 0.0\n",
    "    coverage = ((total_seen - hallucinations) / total_seen) if total_seen > 0 else 0.0\n",
    "    return {\n",
    "        'precision_weighted': pw,\n",
    "        'recall_weighted': rw,\n",
    "        'f1_weighted': fw,\n",
    "        'precision_macro': pm,\n",
    "        'recall_macro': rm,\n",
    "        'f1_macro': fm,\n",
    "        'accuracy': acc,\n",
    "        'samples': len(y_true),\n",
    "        'hallucination_rate': rate,\n",
    "        'coverage': coverage,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "36435a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one(all_json_path: str, gt: Dict[str, Dict[str, str]],\n",
    "                 split_map: Dict[Tuple[str, str], str], subset: Optional[str]) -> Dict[str, float]:\n",
    "    preds = load_model_predictions(all_json_path)\n",
    "\n",
    "    total_seen = 0\n",
    "    total_expected = 0\n",
    "    missing_predictions = 0\n",
    "    for output_name, segs_gt in gt.items():\n",
    "        segs_pred = preds.get(output_name)\n",
    "        for seg_id in segs_gt.keys():\n",
    "            if subset == 'test' and split_map.get((output_name, seg_id), None) != 'test':\n",
    "                continue\n",
    "            total_expected += 1\n",
    "\n",
    "            if segs_pred is None:\n",
    "                missing_predictions += 1\n",
    "                continue\n",
    "\n",
    "            if seg_id not in segs_pred:\n",
    "                missing_predictions += 1\n",
    "                continue\n",
    "\n",
    "            total_seen += 1\n",
    "\n",
    "    y_true, y_pred, hallucinations = collect_pairs(gt, preds, subset, split_map)\n",
    "    result = compute_metrics(y_true, y_pred, hallucinations, total_seen)\n",
    "\n",
    "    inclusive_hallucinations = missing_predictions + hallucinations\n",
    "    if total_expected > 0:\n",
    "        result['hallucination_rate_inclusive'] = inclusive_hallucinations / total_expected\n",
    "        result['coverage_inclusive'] = (total_expected - inclusive_hallucinations) / total_expected\n",
    "    else:\n",
    "        result['hallucination_rate_inclusive'] = 0.0\n",
    "        result['coverage_inclusive'] = 0.0\n",
    "\n",
    "    result['total_expected'] = total_expected\n",
    "    result['total_seen'] = total_seen\n",
    "    result['missing_predictions'] = missing_predictions\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ca3d586b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../results\\llm_models_metrics.csv\n",
      "60\n",
      "                              model_id                   model   context subset  precision_weighted  recall_weighted  f1_weighted  precision_macro  recall_macro  f1_macro  accuracy  samples  hallucination_rate  coverage  hallucination_rate_inclusive  coverage_inclusive  total_expected  total_seen  missing_predictions\n",
      "          deepseek-r1_14b-context-test         deepseek-r1_14b   context   test            0.751245         0.824675     0.786250         0.124266      0.136412  0.130056  0.824675      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "          deepseek-r1_14b-context-full         deepseek-r1_14b   context   full            0.772031         0.843953     0.803660         0.199586      0.178267  0.180558  0.843953      769            0.000000  1.000000                      0.000000            1.000000             769         769                    0\n",
      "          deepseek-r1_32b-context-test         deepseek-r1_32b   context   test            0.746135         0.824675     0.783442         0.123421      0.136412  0.129592  0.824675      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "          deepseek-r1_32b-context-full         deepseek-r1_32b   context   full            0.780139         0.846554     0.807995         0.210253      0.184434  0.186700  0.846554      769            0.000000  1.000000                      0.000000            1.000000             769         769                    0\n",
      "           deepseek-r1_8b-context-test          deepseek-r1_8b   context   test            0.000000         0.000000     0.000000         0.000000      0.000000  0.000000  0.000000       14            0.725490  0.274510                      0.909091            0.090909             154          51                  103\n",
      "           deepseek-r1_8b-context-full          deepseek-r1_8b   context   full            0.003623         0.014493     0.005797         0.025000      0.100000  0.040000  0.014493       69            0.721774  0.278226                      0.910273            0.089727             769         248                  521\n",
      "               gemma3_12b-context-test              gemma3_12b   context   test            0.756103         0.813333     0.782344         0.160072      0.275824  0.185661  0.813333      150            0.019608  0.980392                      0.025974            0.974026             154         153                    1\n",
      "               gemma3_12b-context-full              gemma3_12b   context   full            0.802939         0.823688     0.803579         0.243831      0.282045  0.208380  0.823688      743            0.014589  0.985411                      0.033810            0.966190             769         754                   15\n",
      "               gemma3_27b-context-test              gemma3_27b   context   test            0.751012         0.830065     0.788562         0.123421      0.136412  0.129592  0.830065      153            0.006494  0.993506                      0.006494            0.993506             154         154                    0\n",
      "               gemma3_27b-context-full              gemma3_27b   context   full            0.780733         0.832898     0.798264         0.203776      0.153698  0.158396  0.832898      766            0.002604  0.997396                      0.003901            0.996099             769         768                    1\n",
      "          llama3.1_latest-context-test         llama3.1_latest   context   test            0.816546         0.305195     0.406280         0.177362      0.321111  0.134760  0.305195      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "          llama3.1_latest-context-full         llama3.1_latest   context   full            0.819353         0.305592     0.428449         0.181810      0.169593  0.112888  0.305592      769            0.000000  1.000000                      0.000000            1.000000             769         769                    0\n",
      "            llama4_latest-context-test           llama4_latest   context   test            0.761214         0.837662     0.795729         0.177477      0.175553  0.172672  0.837662      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "            llama4_latest-context-full           llama4_latest   context   full            0.760507         0.850455     0.800990         0.141882      0.145245  0.139746  0.850455      769            0.000000  1.000000                      0.000000            1.000000             769         769                    0\n",
      "      mistral-nemo_latest-context-test     mistral-nemo_latest   context   test            0.755906         0.688312     0.717285         0.129926      0.255639  0.128101  0.688312      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "      mistral-nemo_latest-context-full     mistral-nemo_latest   context   full            0.779500         0.728218     0.748961         0.156725      0.234062  0.141737  0.728218      769            0.000000  1.000000                      0.000000            1.000000             769         769                    0\n",
      "  mistral-small3.2_latest-context-test mistral-small3.2_latest   context   test            0.814170         0.740260     0.774958         0.162755      0.158188  0.159639  0.740260      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "  mistral-small3.2_latest-context-full mistral-small3.2_latest   context   full            0.816145         0.751625     0.780494         0.207218      0.270455  0.217268  0.751625      769            0.000000  1.000000                      0.000000            1.000000             769         769                    0\n",
      "               mistral_7b-context-test              mistral_7b   context   test            0.746929         0.831169     0.786800         0.123552      0.137487  0.130147  0.831169      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "               mistral_7b-context-full              mistral_7b   context   full            0.777932         0.848958     0.805066         0.174124      0.148073  0.146042  0.848958      768            0.001300  0.998700                      0.001300            0.998700             769         769                    0\n",
      "             mixtral_8x7b-context-test            mixtral_8x7b   context   test            0.739415         0.811688     0.773868         0.122309      0.134264  0.128008  0.811688      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "             mixtral_8x7b-context-full            mixtral_8x7b   context   full            0.769548         0.837027     0.800013         0.155960      0.148719  0.147582  0.837027      767            0.002601  0.997399                      0.002601            0.997399             769         769                    0\n",
      "         phi4-mini_latest-context-test        phi4-mini_latest   context   test            0.774560         0.707792     0.739153         0.172570      0.177005  0.172035  0.707792      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "         phi4-mini_latest-context-full        phi4-mini_latest   context   full            0.783461         0.745763     0.763280         0.157577      0.143670  0.147224  0.745763      767            0.002601  0.997399                      0.002601            0.997399             769         769                    0\n",
      "                 phi4_14b-context-test                phi4_14b   context   test            0.838873         0.525974     0.638511         0.192897      0.192022  0.170514  0.525974      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "                 phi4_14b-context-full                phi4_14b   context   full            0.833483         0.544863     0.651433         0.185132      0.208273  0.165170  0.544863      769            0.000000  1.000000                      0.000000            1.000000             769         769                    0\n",
      "                qwen3_14b-context-test               qwen3_14b   context   test            0.816694         0.753247     0.780932         0.183056      0.302119  0.193445  0.753247      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "                qwen3_14b-context-full               qwen3_14b   context   full            0.824783         0.769831     0.792873         0.229203      0.265106  0.202900  0.769831      769            0.000000  1.000000                      0.000000            1.000000             769         769                    0\n",
      "                qwen3_30b-context-test               qwen3_30b   context   test            0.769168         0.779221     0.774121         0.153439      0.152719  0.153010  0.779221      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "                qwen3_30b-context-full               qwen3_30b   context   full            0.783511         0.803641     0.791220         0.250542      0.186503  0.201770  0.803641      769            0.000000  1.000000                      0.000000            1.000000             769         769                    0\n",
      "        deepseek-r1_14b-nocontext-test         deepseek-r1_14b nocontext   test            0.786048         0.828947     0.806399         0.147055      0.147251  0.146217  0.828947      152            0.012987  0.987013                      0.012987            0.987013             154         154                    0\n",
      "        deepseek-r1_14b-nocontext-full         deepseek-r1_14b nocontext   full            0.793857         0.849279     0.815521         0.209044      0.193519  0.189881  0.849279      763            0.007802  0.992198                      0.007802            0.992198             769         769                    0\n",
      "        deepseek-r1_32b-nocontext-test         deepseek-r1_32b nocontext   test            0.786804         0.849673     0.811251         0.172983      0.151548  0.152041  0.849673      153            0.006494  0.993506                      0.006494            0.993506             154         154                    0\n",
      "        deepseek-r1_32b-nocontext-full         deepseek-r1_32b nocontext   full            0.840169         0.859375     0.833119         0.450928      0.295996  0.292359  0.859375      768            0.001300  0.998700                      0.001300            0.998700             769         769                    0\n",
      "         deepseek-r1_8b-nocontext-test          deepseek-r1_8b nocontext   test            0.000000         0.000000     0.000000         0.000000      0.000000  0.000000  0.000000        0            0.000000  0.000000                      1.000000            0.000000             154          72                   82\n",
      "         deepseek-r1_8b-nocontext-full          deepseek-r1_8b nocontext   full            0.000000         0.000000     0.000000         0.000000      0.000000  0.000000  0.000000       10            0.973753  0.026247                      0.986996            0.013004             769         381                  388\n",
      "             gemma3_12b-nocontext-test              gemma3_12b nocontext   test            0.769220         0.803922     0.786188         0.126413      0.132116  0.129202  0.803922      153            0.006494  0.993506                      0.006494            0.993506             154         154                    0\n",
      "             gemma3_12b-nocontext-full              gemma3_12b nocontext   full            0.825917         0.832241     0.820749         0.273226      0.241166  0.229327  0.832241      763            0.006510  0.993490                      0.007802            0.992198             769         768                    1\n",
      "             gemma3_27b-nocontext-test              gemma3_27b nocontext   test            0.788499         0.827815     0.805966         0.182014      0.172912  0.173920  0.827815      151            0.000000  1.000000                      0.019481            0.980519             154         151                    3\n",
      "             gemma3_27b-nocontext-full              gemma3_27b nocontext   full            0.822516         0.849802     0.828579         0.257303      0.221584  0.215222  0.849802      759            0.003937  0.996063                      0.013004            0.986996             769         762                    7\n",
      "            gpt-oss_20b-nocontext-test             gpt-oss_20b nocontext   test            0.816841         0.830882     0.822899         0.157604      0.148104  0.151020  0.830882      136            0.000000  1.000000                      0.116883            0.883117             154         136                   18\n",
      "            gpt-oss_20b-nocontext-full             gpt-oss_20b nocontext   full            0.845845         0.875193     0.856821         0.211798      0.162898  0.171795  0.875193      649            0.000000  1.000000                      0.156047            0.843953             769         649                  120\n",
      "        llama3.1_latest-nocontext-test         llama3.1_latest nocontext   test            0.829939         0.383117     0.515778         0.181784      0.144566  0.135387  0.383117      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "        llama3.1_latest-nocontext-full         llama3.1_latest nocontext   full            0.841356         0.407022     0.536587         0.186913      0.205309  0.143190  0.407022      769            0.000000  1.000000                      0.000000            1.000000             769         769                    0\n",
      "          llama4_latest-nocontext-test           llama4_latest nocontext   test            0.742848         0.798701     0.769763         0.122877      0.132116  0.127329  0.798701      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "          llama4_latest-nocontext-full           llama4_latest nocontext   full            0.794344         0.836387     0.811262         0.215561      0.219936  0.203090  0.836387      764            0.006502  0.993498                      0.006502            0.993498             769         769                    0\n",
      "    mistral-nemo_latest-nocontext-test     mistral-nemo_latest nocontext   test            0.760406         0.720779     0.737198         0.134643      0.261010  0.138623  0.720779      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "    mistral-nemo_latest-nocontext-full     mistral-nemo_latest nocontext   full            0.817220         0.737321     0.767411         0.249862      0.297857  0.200184  0.737321      769            0.000000  1.000000                      0.000000            1.000000             769         769                    0\n",
      "mistral-small3.2_latest-nocontext-test mistral-small3.2_latest nocontext   test            0.783460         0.740260     0.761117         0.145975      0.134362  0.139693  0.740260      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "mistral-small3.2_latest-nocontext-full mistral-small3.2_latest nocontext   full            0.831428         0.780965     0.803489         0.254477      0.306839  0.247896  0.780965      767            0.002601  0.997399                      0.002601            0.997399             769         769                    0\n",
      "             mistral_7b-nocontext-test              mistral_7b nocontext   test            0.843754         0.854305     0.823132         0.313793      0.177068  0.183780  0.854305      151            0.019481  0.980519                      0.019481            0.980519             154         154                    0\n",
      "             mistral_7b-nocontext-full              mistral_7b nocontext   full            0.811328         0.848168     0.813271         0.315196      0.210354  0.213138  0.848168      764            0.006502  0.993498                      0.006502            0.993498             769         769                    0\n",
      "       phi4-mini_latest-nocontext-test        phi4-mini_latest nocontext   test            0.800459         0.607843     0.681220         0.154790      0.310956  0.153826  0.607843      153            0.006494  0.993506                      0.006494            0.993506             154         154                    0\n",
      "       phi4-mini_latest-nocontext-full        phi4-mini_latest nocontext   full            0.808578         0.627937     0.700127         0.176531      0.257847  0.157790  0.627937      766            0.003901  0.996099                      0.003901            0.996099             769         769                    0\n",
      "               phi4_14b-nocontext-test                phi4_14b nocontext   test            0.789847         0.779221     0.784138         0.198052      0.187360  0.191935  0.779221      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "               phi4_14b-nocontext-full                phi4_14b nocontext   full            0.822298         0.790637     0.804489         0.239502      0.261237  0.234412  0.790637      769            0.000000  1.000000                      0.000000            1.000000             769         769                    0\n",
      "              qwen3_14b-nocontext-test               qwen3_14b nocontext   test            0.797453         0.831169     0.810176         0.255218      0.303095  0.261296  0.831169      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "              qwen3_14b-nocontext-full               qwen3_14b nocontext   full            0.806131         0.836151     0.817221         0.203152      0.292627  0.206121  0.836151      769            0.000000  1.000000                      0.000000            1.000000             769         769                    0\n",
      "              qwen3_30b-nocontext-test               qwen3_30b nocontext   test            0.791330         0.843137     0.810760         0.173727      0.150474  0.151960  0.843137      153            0.006494  0.993506                      0.006494            0.993506             154         154                    0\n",
      "              qwen3_30b-nocontext-full               qwen3_30b nocontext   full            0.815735         0.848958     0.818654         0.287239      0.206333  0.212811  0.848958      768            0.001300  0.998700                      0.001300            0.998700             769         769                    0\n",
      "\n",
      "Rankings by context and subset:\n",
      "\n",
      "Ranking - with context and full data (rows=15)\n",
      "                            model_id                   model context subset  precision_weighted  recall_weighted  f1_weighted  precision_macro  recall_macro  f1_macro  accuracy  samples  hallucination_rate  coverage  hallucination_rate_inclusive  coverage_inclusive  total_expected  total_seen  missing_predictions\n",
      "mistral-small3.2_latest-context-full mistral-small3.2_latest context   full            0.816145         0.751625     0.780494         0.207218      0.270455  0.217268  0.751625      769            0.000000  1.000000                      0.000000            1.000000             769         769                    0\n",
      "             gemma3_12b-context-full              gemma3_12b context   full            0.802939         0.823688     0.803579         0.243831      0.282045  0.208380  0.823688      743            0.014589  0.985411                      0.033810            0.966190             769         754                   15\n",
      "              qwen3_14b-context-full               qwen3_14b context   full            0.824783         0.769831     0.792873         0.229203      0.265106  0.202900  0.769831      769            0.000000  1.000000                      0.000000            1.000000             769         769                    0\n",
      "              qwen3_30b-context-full               qwen3_30b context   full            0.783511         0.803641     0.791220         0.250542      0.186503  0.201770  0.803641      769            0.000000  1.000000                      0.000000            1.000000             769         769                    0\n",
      "        deepseek-r1_32b-context-full         deepseek-r1_32b context   full            0.780139         0.846554     0.807995         0.210253      0.184434  0.186700  0.846554      769            0.000000  1.000000                      0.000000            1.000000             769         769                    0\n",
      "        deepseek-r1_14b-context-full         deepseek-r1_14b context   full            0.772031         0.843953     0.803660         0.199586      0.178267  0.180558  0.843953      769            0.000000  1.000000                      0.000000            1.000000             769         769                    0\n",
      "               phi4_14b-context-full                phi4_14b context   full            0.833483         0.544863     0.651433         0.185132      0.208273  0.165170  0.544863      769            0.000000  1.000000                      0.000000            1.000000             769         769                    0\n",
      "             gemma3_27b-context-full              gemma3_27b context   full            0.780733         0.832898     0.798264         0.203776      0.153698  0.158396  0.832898      766            0.002604  0.997396                      0.003901            0.996099             769         768                    1\n",
      "           mixtral_8x7b-context-full            mixtral_8x7b context   full            0.769548         0.837027     0.800013         0.155960      0.148719  0.147582  0.837027      767            0.002601  0.997399                      0.002601            0.997399             769         769                    0\n",
      "       phi4-mini_latest-context-full        phi4-mini_latest context   full            0.783461         0.745763     0.763280         0.157577      0.143670  0.147224  0.745763      767            0.002601  0.997399                      0.002601            0.997399             769         769                    0\n",
      "             mistral_7b-context-full              mistral_7b context   full            0.777932         0.848958     0.805066         0.174124      0.148073  0.146042  0.848958      768            0.001300  0.998700                      0.001300            0.998700             769         769                    0\n",
      "    mistral-nemo_latest-context-full     mistral-nemo_latest context   full            0.779500         0.728218     0.748961         0.156725      0.234062  0.141737  0.728218      769            0.000000  1.000000                      0.000000            1.000000             769         769                    0\n",
      "          llama4_latest-context-full           llama4_latest context   full            0.760507         0.850455     0.800990         0.141882      0.145245  0.139746  0.850455      769            0.000000  1.000000                      0.000000            1.000000             769         769                    0\n",
      "        llama3.1_latest-context-full         llama3.1_latest context   full            0.819353         0.305592     0.428449         0.181810      0.169593  0.112888  0.305592      769            0.000000  1.000000                      0.000000            1.000000             769         769                    0\n",
      "         deepseek-r1_8b-context-full          deepseek-r1_8b context   full            0.003623         0.014493     0.005797         0.025000      0.100000  0.040000  0.014493       69            0.721774  0.278226                      0.910273            0.089727             769         248                  521\n",
      "Saved ranking CSV: ../results\\llm_models_metrics_ranking_context_full.csv\n",
      "\n",
      "Ranking - with context and test data (rows=15)\n",
      "                            model_id                   model context subset  precision_weighted  recall_weighted  f1_weighted  precision_macro  recall_macro  f1_macro  accuracy  samples  hallucination_rate  coverage  hallucination_rate_inclusive  coverage_inclusive  total_expected  total_seen  missing_predictions\n",
      "              qwen3_14b-context-test               qwen3_14b context   test            0.816694         0.753247     0.780932         0.183056      0.302119  0.193445  0.753247      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "             gemma3_12b-context-test              gemma3_12b context   test            0.756103         0.813333     0.782344         0.160072      0.275824  0.185661  0.813333      150            0.019608  0.980392                      0.025974            0.974026             154         153                    1\n",
      "          llama4_latest-context-test           llama4_latest context   test            0.761214         0.837662     0.795729         0.177477      0.175553  0.172672  0.837662      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "       phi4-mini_latest-context-test        phi4-mini_latest context   test            0.774560         0.707792     0.739153         0.172570      0.177005  0.172035  0.707792      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "               phi4_14b-context-test                phi4_14b context   test            0.838873         0.525974     0.638511         0.192897      0.192022  0.170514  0.525974      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "mistral-small3.2_latest-context-test mistral-small3.2_latest context   test            0.814170         0.740260     0.774958         0.162755      0.158188  0.159639  0.740260      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "              qwen3_30b-context-test               qwen3_30b context   test            0.769168         0.779221     0.774121         0.153439      0.152719  0.153010  0.779221      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "        llama3.1_latest-context-test         llama3.1_latest context   test            0.816546         0.305195     0.406280         0.177362      0.321111  0.134760  0.305195      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "             mistral_7b-context-test              mistral_7b context   test            0.746929         0.831169     0.786800         0.123552      0.137487  0.130147  0.831169      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "        deepseek-r1_14b-context-test         deepseek-r1_14b context   test            0.751245         0.824675     0.786250         0.124266      0.136412  0.130056  0.824675      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "        deepseek-r1_32b-context-test         deepseek-r1_32b context   test            0.746135         0.824675     0.783442         0.123421      0.136412  0.129592  0.824675      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "             gemma3_27b-context-test              gemma3_27b context   test            0.751012         0.830065     0.788562         0.123421      0.136412  0.129592  0.830065      153            0.006494  0.993506                      0.006494            0.993506             154         154                    0\n",
      "    mistral-nemo_latest-context-test     mistral-nemo_latest context   test            0.755906         0.688312     0.717285         0.129926      0.255639  0.128101  0.688312      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "           mixtral_8x7b-context-test            mixtral_8x7b context   test            0.739415         0.811688     0.773868         0.122309      0.134264  0.128008  0.811688      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "         deepseek-r1_8b-context-test          deepseek-r1_8b context   test            0.000000         0.000000     0.000000         0.000000      0.000000  0.000000  0.000000       14            0.725490  0.274510                      0.909091            0.090909             154          51                  103\n",
      "Saved ranking CSV: ../results\\llm_models_metrics_ranking_context_test.csv\n",
      "\n",
      "Ranking - without context and full data (rows=15)\n",
      "                              model_id                   model   context subset  precision_weighted  recall_weighted  f1_weighted  precision_macro  recall_macro  f1_macro  accuracy  samples  hallucination_rate  coverage  hallucination_rate_inclusive  coverage_inclusive  total_expected  total_seen  missing_predictions\n",
      "        deepseek-r1_32b-nocontext-full         deepseek-r1_32b nocontext   full            0.840169         0.859375     0.833119         0.450928      0.295996  0.292359  0.859375      768            0.001300  0.998700                      0.001300            0.998700             769         769                    0\n",
      "mistral-small3.2_latest-nocontext-full mistral-small3.2_latest nocontext   full            0.831428         0.780965     0.803489         0.254477      0.306839  0.247896  0.780965      767            0.002601  0.997399                      0.002601            0.997399             769         769                    0\n",
      "               phi4_14b-nocontext-full                phi4_14b nocontext   full            0.822298         0.790637     0.804489         0.239502      0.261237  0.234412  0.790637      769            0.000000  1.000000                      0.000000            1.000000             769         769                    0\n",
      "             gemma3_12b-nocontext-full              gemma3_12b nocontext   full            0.825917         0.832241     0.820749         0.273226      0.241166  0.229327  0.832241      763            0.006510  0.993490                      0.007802            0.992198             769         768                    1\n",
      "             gemma3_27b-nocontext-full              gemma3_27b nocontext   full            0.822516         0.849802     0.828579         0.257303      0.221584  0.215222  0.849802      759            0.003937  0.996063                      0.013004            0.986996             769         762                    7\n",
      "             mistral_7b-nocontext-full              mistral_7b nocontext   full            0.811328         0.848168     0.813271         0.315196      0.210354  0.213138  0.848168      764            0.006502  0.993498                      0.006502            0.993498             769         769                    0\n",
      "              qwen3_30b-nocontext-full               qwen3_30b nocontext   full            0.815735         0.848958     0.818654         0.287239      0.206333  0.212811  0.848958      768            0.001300  0.998700                      0.001300            0.998700             769         769                    0\n",
      "              qwen3_14b-nocontext-full               qwen3_14b nocontext   full            0.806131         0.836151     0.817221         0.203152      0.292627  0.206121  0.836151      769            0.000000  1.000000                      0.000000            1.000000             769         769                    0\n",
      "          llama4_latest-nocontext-full           llama4_latest nocontext   full            0.794344         0.836387     0.811262         0.215561      0.219936  0.203090  0.836387      764            0.006502  0.993498                      0.006502            0.993498             769         769                    0\n",
      "    mistral-nemo_latest-nocontext-full     mistral-nemo_latest nocontext   full            0.817220         0.737321     0.767411         0.249862      0.297857  0.200184  0.737321      769            0.000000  1.000000                      0.000000            1.000000             769         769                    0\n",
      "        deepseek-r1_14b-nocontext-full         deepseek-r1_14b nocontext   full            0.793857         0.849279     0.815521         0.209044      0.193519  0.189881  0.849279      763            0.007802  0.992198                      0.007802            0.992198             769         769                    0\n",
      "            gpt-oss_20b-nocontext-full             gpt-oss_20b nocontext   full            0.845845         0.875193     0.856821         0.211798      0.162898  0.171795  0.875193      649            0.000000  1.000000                      0.156047            0.843953             769         649                  120\n",
      "       phi4-mini_latest-nocontext-full        phi4-mini_latest nocontext   full            0.808578         0.627937     0.700127         0.176531      0.257847  0.157790  0.627937      766            0.003901  0.996099                      0.003901            0.996099             769         769                    0\n",
      "        llama3.1_latest-nocontext-full         llama3.1_latest nocontext   full            0.841356         0.407022     0.536587         0.186913      0.205309  0.143190  0.407022      769            0.000000  1.000000                      0.000000            1.000000             769         769                    0\n",
      "         deepseek-r1_8b-nocontext-full          deepseek-r1_8b nocontext   full            0.000000         0.000000     0.000000         0.000000      0.000000  0.000000  0.000000       10            0.973753  0.026247                      0.986996            0.013004             769         381                  388\n",
      "Saved ranking CSV: ../results\\llm_models_metrics_ranking_nocontext_full.csv\n",
      "\n",
      "Ranking - without context and test data (rows=15)\n",
      "                              model_id                   model   context subset  precision_weighted  recall_weighted  f1_weighted  precision_macro  recall_macro  f1_macro  accuracy  samples  hallucination_rate  coverage  hallucination_rate_inclusive  coverage_inclusive  total_expected  total_seen  missing_predictions\n",
      "              qwen3_14b-nocontext-test               qwen3_14b nocontext   test            0.797453         0.831169     0.810176         0.255218      0.303095  0.261296  0.831169      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "               phi4_14b-nocontext-test                phi4_14b nocontext   test            0.789847         0.779221     0.784138         0.198052      0.187360  0.191935  0.779221      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "             mistral_7b-nocontext-test              mistral_7b nocontext   test            0.843754         0.854305     0.823132         0.313793      0.177068  0.183780  0.854305      151            0.019481  0.980519                      0.019481            0.980519             154         154                    0\n",
      "             gemma3_27b-nocontext-test              gemma3_27b nocontext   test            0.788499         0.827815     0.805966         0.182014      0.172912  0.173920  0.827815      151            0.000000  1.000000                      0.019481            0.980519             154         151                    3\n",
      "       phi4-mini_latest-nocontext-test        phi4-mini_latest nocontext   test            0.800459         0.607843     0.681220         0.154790      0.310956  0.153826  0.607843      153            0.006494  0.993506                      0.006494            0.993506             154         154                    0\n",
      "        deepseek-r1_32b-nocontext-test         deepseek-r1_32b nocontext   test            0.786804         0.849673     0.811251         0.172983      0.151548  0.152041  0.849673      153            0.006494  0.993506                      0.006494            0.993506             154         154                    0\n",
      "              qwen3_30b-nocontext-test               qwen3_30b nocontext   test            0.791330         0.843137     0.810760         0.173727      0.150474  0.151960  0.843137      153            0.006494  0.993506                      0.006494            0.993506             154         154                    0\n",
      "            gpt-oss_20b-nocontext-test             gpt-oss_20b nocontext   test            0.816841         0.830882     0.822899         0.157604      0.148104  0.151020  0.830882      136            0.000000  1.000000                      0.116883            0.883117             154         136                   18\n",
      "        deepseek-r1_14b-nocontext-test         deepseek-r1_14b nocontext   test            0.786048         0.828947     0.806399         0.147055      0.147251  0.146217  0.828947      152            0.012987  0.987013                      0.012987            0.987013             154         154                    0\n",
      "mistral-small3.2_latest-nocontext-test mistral-small3.2_latest nocontext   test            0.783460         0.740260     0.761117         0.145975      0.134362  0.139693  0.740260      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "    mistral-nemo_latest-nocontext-test     mistral-nemo_latest nocontext   test            0.760406         0.720779     0.737198         0.134643      0.261010  0.138623  0.720779      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "        llama3.1_latest-nocontext-test         llama3.1_latest nocontext   test            0.829939         0.383117     0.515778         0.181784      0.144566  0.135387  0.383117      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "             gemma3_12b-nocontext-test              gemma3_12b nocontext   test            0.769220         0.803922     0.786188         0.126413      0.132116  0.129202  0.803922      153            0.006494  0.993506                      0.006494            0.993506             154         154                    0\n",
      "          llama4_latest-nocontext-test           llama4_latest nocontext   test            0.742848         0.798701     0.769763         0.122877      0.132116  0.127329  0.798701      154            0.000000  1.000000                      0.000000            1.000000             154         154                    0\n",
      "         deepseek-r1_8b-nocontext-test          deepseek-r1_8b nocontext   test            0.000000         0.000000     0.000000         0.000000      0.000000  0.000000  0.000000        0            0.000000  0.000000                      1.000000            0.000000             154          72                   82\n",
      "Saved ranking CSV: ../results\\llm_models_metrics_ranking_nocontext_test.csv\n",
      "Saved combined rankings CSV: ../results\\llm_models_metrics_rankings_all.csv\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    base_results_dir = os.path.join('../results')\n",
    "    gt_path = os.path.join(base_results_dir, 'resultado_manual.json')\n",
    "    splits_csv = os.path.join(base_results_dir, 'data_splits.csv')\n",
    "\n",
    "    if not os.path.exists(gt_path):\n",
    "        raise FileNotFoundError(f\"Ground truth not found at {gt_path}\")\n",
    "    if not os.path.exists(splits_csv):\n",
    "        raise FileNotFoundError(f\"Splits CSV not found at {splits_csv}\")\n",
    "\n",
    "    gt = load_ground_truth(gt_path)\n",
    "    split_map = load_splits(splits_csv)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    search_specs = [\n",
    "        ('resultados_com_contexto', 'context'),\n",
    "        ('resultados_sem_contexto', 'nocontext'),\n",
    "    ]\n",
    "\n",
    "    for subdir, ctx in search_specs:\n",
    "        ctx_dir = os.path.join(base_results_dir, subdir)\n",
    "        if not os.path.isdir(ctx_dir):\n",
    "            continue\n",
    "\n",
    "        for name in sorted(os.listdir(ctx_dir)):\n",
    "            model_dir = os.path.join(ctx_dir, name)\n",
    "            if not os.path.isdir(model_dir):\n",
    "                continue\n",
    "\n",
    "            json_candidates = glob.glob(os.path.join(model_dir, 'all_*.json'))\n",
    "            if not json_candidates:\n",
    "                continue\n",
    "\n",
    "            all_json = json_candidates[0]\n",
    "\n",
    "            model_name = name\n",
    "\n",
    "            for subset in ('test', 'full'):\n",
    "                subset_arg = None if subset == 'full' else 'test'\n",
    "                metrics = evaluate_one(all_json, gt, split_map, subset_arg)\n",
    "\n",
    "                row_id = f\"{model_name}-{ctx}-{subset}\"\n",
    "                row = {\n",
    "                    'model_id': row_id,\n",
    "                    'model': model_name,\n",
    "                    'context': ctx,\n",
    "                    'subset': subset,\n",
    "                }\n",
    "                for k, v in metrics.items():\n",
    "                    row[k] = round(float(v), 6) if isinstance(v, float) else v\n",
    "                rows.append(row)\n",
    "\n",
    "    if not rows:\n",
    "        print('No results found. Ensure result folders contain all_*.json files.')\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    subset_order = {'test': 0, 'full': 1}\n",
    "    df['subset_ord'] = df['subset'].map(subset_order)\n",
    "    df = df.sort_values(by=['context', 'model', 'subset_ord']).drop(columns=['subset_ord'])\n",
    "\n",
    "    base_cols = ['model_id', 'model', 'context', 'subset']\n",
    "    metric_cols = [\n",
    "        'precision_weighted', 'recall_weighted', 'f1_weighted',\n",
    "        'precision_macro', 'recall_macro', 'f1_macro',\n",
    "        'accuracy', 'samples', 'hallucination_rate', 'coverage',\n",
    "        'hallucination_rate_inclusive', 'coverage_inclusive',\n",
    "        'total_expected', 'total_seen', 'missing_predictions'\n",
    "    ]\n",
    "    out_cols = base_cols + metric_cols\n",
    "    for c in metric_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = 0.0\n",
    "    df = df[out_cols]\n",
    "\n",
    "    out_csv = os.path.join(base_results_dir, 'llm_models_metrics.csv')\n",
    "    df.to_csv(out_csv, index=False, encoding='utf-8')\n",
    "    print(f\"Saved: {out_csv}\")\n",
    "    print(len(df))\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nRankings by context and subset:\")\n",
    "\n",
    "    # Ensure numeric type for samples in case it was parsed as string\n",
    "    df['samples'] = pd.to_numeric(df['samples'], errors='coerce')\n",
    "\n",
    "    #expected_samples = { 'test': 154, 'full': 769 }\n",
    "\n",
    "    categories = [\n",
    "        (\"with context and full data\", 'context', 'full'),\n",
    "        (\"with context and test data\", 'context', 'test'),\n",
    "        (\"without context and full data\", 'nocontext', 'full'),\n",
    "        (\"without context and test data\", 'nocontext', 'test'),\n",
    "    ]\n",
    "\n",
    "    all_rankings = []\n",
    "\n",
    "    for title, ctx_val, subset in categories:\n",
    "        mask = (\n",
    "            (df['context'] == ctx_val) &\n",
    "            (df['subset'] == subset) #&\n",
    "            #(df['samples'] == expected_samples[subset])\n",
    "        )\n",
    "        df_cat = df.loc[mask].sort_values(by='f1_macro', ascending=False)\n",
    "        print(f\"\\nRanking - {title} (rows={len(df_cat)})\")\n",
    "        if len(df_cat) == 0:\n",
    "            print(\"No results found for this category.\")\n",
    "        else:\n",
    "            print(df_cat.to_string(index=False))\n",
    "\n",
    "        # Save per-category ranking CSV\n",
    "        out_csv_cat = os.path.join(base_results_dir, f\"llm_models_metrics_ranking_{ctx_val}_{subset}.csv\")\n",
    "        df_cat.to_csv(out_csv_cat, index=False, encoding='utf-8')\n",
    "        print(f\"Saved ranking CSV: {out_csv_cat}\")\n",
    "\n",
    "        # Accumulate for combined rankings file\n",
    "        tmp = df_cat.copy()\n",
    "        tmp['ranking_title'] = title\n",
    "        tmp['ranking_context'] = ctx_val\n",
    "        tmp['ranking_subset'] = subset\n",
    "        all_rankings.append(tmp)\n",
    "\n",
    "    # Save combined rankings CSV\n",
    "    if all_rankings:\n",
    "        df_all = pd.concat(all_rankings, ignore_index=True)\n",
    "        out_all_csv = os.path.join(base_results_dir, 'llm_models_metrics_rankings_all.csv')\n",
    "        df_all.to_csv(out_all_csv, index=False, encoding='utf-8')\n",
    "        print(f\"Saved combined rankings CSV: {out_all_csv}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
