# Emotion Recognition in Agile Software Meetings: A Comparative Study of ML, DL, and Text-based LLM Approaches

This repository accompanies the paper “Emotion Recognition in Agile Software Meetings: A Comparative Study of ML, DL, and Text-based LLM Approaches”.
It provides the complementary materials and scripts used to evaluate Machine Learning (ML), Deep Learning (DL),
and text-based Large Language Models (LLMs) for emotion classification in audio segments from real agile software development meetings.

## Overview

- Ground truth and splits: `src/LLMs/results/manual_labelling.json` and `src/LLMs/results/data_splits.csv`.
- Model predictions: `src/LLMs/results/results_ctxt/<model>/all_*.json` and `src/LLMs/results/results_no_ctxt/<model>/all_*.json`.
- Notebook `src/LLMs/tools/generate_llm_metrics.ipynb` consolidates per‑model metrics and saves `llm_models_metrics.csv` and rankings.
- Utility scripts compute average context deltas, per‑emotion metrics, and text summaries.

## Layout (short)

- `data/` and `recordings/`: audio and transcripts.
- `src/LLMs/results/`
		- `manual_labelling.json`: ground truth.
	    - `data_splits.csv`: maps segments to splits (train/test).
		- `results_ctxt/` and `results_no_ctxt/`: per‑model outputs (one folder per model, `all_*.json`).
	    - Reports generated by scripts are also written here.
- `src/LLMs/tools/`: metrics notebook and scripts.

## Prerequisites

- Python 3.10+ (tested with 3.13) and `pip`.
- Install dependencies:

```powershell
pip install -r src/LLMs/requirements.txt
```

## Generate consolidated metrics (CSV + rankings)

1) Open `src/LLMs/tools/generate_llm_metrics.ipynb` in VS Code/Jupyter and run the cells. It will:
	 - Load GT and splits.
	 - Traverse model results (with and without context).
	 - Compute metrics for two subsets:
		 - `full` (all GT segments)
		 - `test` (only segments marked “test” in the CSV)
	 - Save:
		 - `src/LLMs/results/llm_models_metrics.csv`
		 - Rankings per category:
			 - `llm_models_metrics_ranking_context_full.csv`
			 - `llm_models_metrics_ranking_context_test.csv`
			 - `llm_models_metrics_ranking_nocontext_full.csv`
			 - `llm_models_metrics_ranking_nocontext_test.csv`
			 - Combined: `llm_models_metrics_rankings_all.csv`

2) The notebook also prints rankings sorted by `f1_macro`.

## Reporting scripts

All commands assume the repo root as current working directory.

### 1) Average context effect per model

File: `src/LLMs/tools/compare_context_effect.py`

Reads `llm_models_metrics.csv` and prints:
- Average delta of `f1_macro` (context − nocontext) per subset (full/test).
- Per‑model list (F1 with/without context, delta and %). Values rounded to 2 decimals.

```powershell
python "src/LLMs/tools/compare_context_effect.py"
```

### 2) Text report per model (macro/weighted + coverage)

File: `src/LLMs/tools/export_model_metrics_text.py`

Generates `src/LLMs/results/model_metrics_report.txt` with, for each model:
- Full dataset (with/without context): macro/weighted precision/recall/F1 and coverage (coverage_inclusive) in %.
- Test split (with/without context): macro/weighted precision/recall/F1 and coverage (coverage_inclusive) in %.

```powershell
python "src/LLMs/tools/export_model_metrics_text.py"
```

### 3) Per‑emotion metrics (precision/recall/F1)

File: `src/LLMs/tools/compare_per_emotion.py`

Outputs:
- Base per emotion: `src/LLMs/results/per_emotion_metrics.csv` with `model, context, subset, emotion, precision, recall, f1, support`.
- Context comparison: `src/LLMs/results/per_emotion_metrics_comparison.csv` joining with/without context by `model, subset, emotion`, with deltas and %.
- Wide format: `src/LLMs/results/per_emotion_metrics_wide.csv` with columns for (full/test) × (context/nocontext) of precision/recall/F1 by `model` and `emotion`.
- Prints a short summary of top improvements per emotion.

Usage:

```powershell
python "src/LLMs/tools/compare_per_emotion.py"

# optional: print the per‑emotion table for a specific model
python "src/LLMs/tools/compare_per_emotion.py" --print-model qwen3_14b
```

## Metric definitions (short)

- `total_expected`: number of GT segments considered in the subset (full: all; test: only “test”).
- `total_seen`: number of segments for which a prediction exists.
- `missing_predictions`: expected segments with no prediction.
- `hallucinations`: segments with a present prediction but no valid label (not mappable to canonical labels).
- `samples`: valid pairs used by metrics = `total_seen − hallucinations`.
- `coverage`: `samples / total_seen` (among seen, usable fraction).
- `coverage_inclusive`: `samples / total_expected` (among all expected, usable fraction) – this is the reported coverage (%).
- `hallucination_rate`: `hallucinations / total_seen`.
- `hallucination_rate_inclusive`: `(missing_predictions + hallucinations) / total_expected`.
- Macro/weighted metrics and `accuracy` are computed with scikit‑learn on valid `y_true/y_pred` pairs, using `zero_division=0`.

Canonical labels: `Happiness, Sadness, Fear, Anger, Surprise, Disgust, Neutral`.